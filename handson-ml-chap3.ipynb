{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Basic ML Classification\nThis notebook is based on `Chapter 3 - Classification` of Hands-On ML, which uses the standard MNIST dataset"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# common imports\nimport sys\nimport sklearn\nimport numpy as np\nimport os\nimport pandas as pd\nfrom pathlib import Path\n\n# Setting seed value \nnp.random.seed(42)\n\n#figures\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n# Sets defaults/can also be imported from a style file\nmpl.rc('axes', labelsize=12)\nmpl.rc('xtick', labelsize=10)\nmpl.rc('ytick', labelsize=10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Saving path to a directory\npath = Path('../input/Kannada-MNIST/')\nfor file in path.iterdir():\n    print(file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(path/'train.csv', low_memory=False)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading dataset into separate numpy arrays\nx = df.iloc[:,1:].values\ny = df.iloc[:,0].values\ny[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of unique classes in the dataset\nnp.unique(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Similar to MNIST image shape\nx.shape, y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Viewing the digits in the dataset\n# Aren't very clea\ndigit = x[np.random.randint(0, 100)].reshape(28, 28)\nplt.imshow(digit, cmap=mpl.cm.binary, interpolation='nearest')\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting images of each uniques class in a row\ndef plot_digits(images, row_size):\n    n_rows = (images.shape[0]) // row_size\n    row_images = []\n    for row in range(n_rows):\n        rimages = images[row * row_size : (row + 1) * row_size]\n        row_images.append(np.concatenate(rimages, axis=1))\n    image = np.concatenate(row_images, axis=0)\n    plt.imshow(image, cmap=mpl.cm.binary)\n    plt.axis(\"off\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nThe plot of images shows that the labels are in a sequential order\nSo we need to randomize the data before passing through a NN model\n'''\nplt.figure(figsize=(9,9))\nexample_images = x[:100].reshape(-1, 28, 28)\nplot_digits(example_images, 10)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Shuffling the data \n# This creates a random permutation of indices in the dataset\nindices = np.random.permutation(x.shape[0])\nx = x[indices]\ny = y[indices]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now the first 100 points have been shuffled\nplt.figure(figsize=(9,9))\nexample_images = x[:100].reshape(-1, 28, 28)\nplot_digits(example_images, 10)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train-Test Split\n# Using Stratified Split allows to have the sampe proportion of the data in both train and test set\nfrom sklearn.model_selection import StratifiedShuffleSplit\nss = StratifiedShuffleSplit(n_splits=1, test_size=0.20, random_state=42)\nfor train_ind, test_ind in ss.split(x, y):\n    x_train, x_test = x[train_ind], x[test_ind]\n    y_train, y_test = y[train_ind], y[test_ind]\nprint(\"Train Shape:\", x_train.shape)\nprint(\"Test Shape:\", x_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nAfter train test split the proportion of the classes remains the same\n'''\nprint(\"Proportion of classes in original data:\", np.unique(y, return_counts=True)[1] / y.shape[0])\nprint(\"Proportion of classes in training data:\", np.unique(y_train, return_counts=True)[1] / y_train.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\nfrom sklearn.linear_model import SGDClassifier\n# Hinge loss which is the default loss function uses the SVM Classifier\nsgd = SGDClassifier(n_jobs=-1, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Confusion Matrix for complete dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# For confusion matrix on complete data\nfrom sklearn.model_selection import cross_val_predict\ny_pred = cross_val_predict(sgd, x, y , cv=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_confusion_matrix(df, col=plt.cm.gray):\n    fig = plt.figure(figsize=(6,6))\n    ax = fig.add_subplot(111)\n    cax = ax.matshow(df, cmap=col)\n    fig.colorbar(cax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The SGD Classifier works well as very few classes are wrongly predicted\nplot_confusion_matrix(confusion_matrix(y, y_pred), plt.cm.plasma)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Binary classifier for Precision Recall Curve"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Setting up as binary classifier as precision-recall can't be plot for multiclass\n# Taking similar to book for number 5\ny_5 = (y==5)\ny_pred_5 = cross_val_predict(sgd, x, y_5 , cv=5, method=\"decision_function\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve\nprecisions, recalls, thresholds = precision_recall_curve(y_5, y_pred_5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\", linewidth=2)\n    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\", linewidth=2)\n    plt.legend(loc=\"center right\", fontsize=16) # Not shown in the book\n    plt.xlabel(\"Threshold\", fontsize=16)        # Not shown\n    plt.grid(True)                              # Not shown\n    plt.axis([-50000, 50000, 0, 1])             # Not shown","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"recall_90_precision = recalls[np.argmax(precisions >= 0.90)]\nthreshold_90_precision = thresholds[np.argmax(precisions >= 0.90)]\n\n\nplt.figure(figsize=(8, 4))                                                                  # Not shown\nplot_precision_recall_vs_threshold(precisions, recalls, thresholds)\nplt.plot([threshold_90_precision, threshold_90_precision], [0., 0.9], \"r:\")                 # Not shown\nplt.plot([-50000, threshold_90_precision], [0.9, 0.9], \"r:\")                                # Not shown\nplt.plot([-50000, threshold_90_precision], [recall_90_precision, recall_90_precision], \"r:\")# Not shown\nplt.plot([threshold_90_precision], [0.9], \"ro\")                                             # Not shown\nplt.plot([threshold_90_precision], [recall_90_precision], \"ro\")                             # Not shown\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We usually select the point just before the sharp drop i.e recall=0.9 and precision=0.95\n# Our model works well as the precision recall curve is steep and since our data is imbalances, it works better\ndef plot_precision_vs_recall(precisions, recalls):\n    plt.plot(recalls, precisions, \"b-\", linewidth=2)\n    plt.xlabel(\"Recall\", fontsize=16)\n    plt.ylabel(\"Precision\", fontsize=16)\n    plt.axis([0, 1, 0, 1])\n    plt.grid(True)\n\nplt.figure(figsize=(8, 6))\nplot_precision_vs_recall(precisions, recalls)\nplt.plot([0.90, 0.90], [0., 0.95], \"r:\")\nplt.plot([0.0, 0.9], [0.95, 0.95], \"r:\")\nplt.plot([0.9], [0.95], \"ro\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ROC curve"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve\n# true positive rate is recall\nfpr, tpr, thresholds = roc_curve(y_5, y_pred_5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here we want our curve to be as steep as possible initially\ndef plot_roc_curve(fpr, tpr, label=None):\n    plt.plot(fpr, tpr, linewidth=2, label=label)\n    plt.plot([0, 1], [0, 1], 'k--') # dashed diagonal\n    plt.axis([0, 1, 0, 1])                                    # Not shown in the book\n    plt.xlabel('False Positive Rate (Fall-Out)', fontsize=16) # Not shown\n    plt.ylabel('True Positive Rate (Recall)', fontsize=16)    # Not shown\n    plt.grid(True)                                            # Not shown\n\nplt.figure(figsize=(8, 6))                         # Not shown\nplot_roc_curve(fpr, tpr)\nplt.plot([4.837e-3, 4.837e-3], [0., 0.4368], \"r:\") # Not shown\nplt.plot([0.0, 4.837e-3], [0.4368, 0.4368], \"r:\")  # Not shown\nplt.plot([4.837e-3], [0.4368], \"ro\")               # Not shown\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\n\nroc_auc_score(y_5, y_pred_5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Error Analysis for multiclass classification"},{"metadata":{"trusted":true},"cell_type":"code","source":"conf_sum = confusion_matrix(y, y_pred)\nrow_sums = conf_sum.sum(axis=1, keepdims=True)\nnorm_conf = conf_sum / row_sums","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.fill_diagonal(norm_conf, 0)\nplt.matshow(norm_conf, cmap=plt.cm.gray)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cl_3, cl_7 = 3, 7\nX_33 = x[(y == cl_3) & (y_pred == cl_3)]\nX_37 = x[(y == cl_3) & (y_pred == cl_7)]\nX_73 = x[(y == cl_7) & (y_pred == cl_3)]\nX_77 = x[(y == cl_7) & (y_pred == cl_7)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 3 and 7 both look similar with rounding in 3 in lower part and 7 having round in upper part\n# Also great way of using subplots\nplt.figure(figsize=(8,8))\nplt.subplot(221); plot_digits(X_33[:25].reshape(-1, 28, 28), 5)\nplt.subplot(222); plot_digits(X_37[:25].reshape(-1, 28, 28), 5)\nplt.subplot(223); plot_digits(X_73[:25].reshape(-1, 28, 28), 5)\nplt.subplot(224); plot_digits(X_77[:25].reshape(-1, 28, 28), 5)\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}